\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr2014authorKit_latex/cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{array}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,urlcolor=blue,bookmarks=false]{hyperref}

%\cvprfinalcopy

\def\cvprPaperID{652}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{\mbox{}\vspace{-1cm}\\Good practice for efficient action
recognition\vspace{-.2cm}\\}

\author{Vadim Kantorov\qquad Ivan Laptev\\INRIA - WILLOW / Dept.
of Computer Science∗, Ecole Normale Supérieure}

\maketitle
%\thispagestyle{empty}

\begin{abstract}
\mbox{}\vspace{-.6cm}\\
Abstract here
\end{abstract}

\mbox{}\vspace{-1.3cm}\\


\section{Introduction}

The amount of video has increased dramatically over recent years
and continues to grow. Striking examples of this development
include 6000 years of video uploaded to YouTube
yearly\footnote{\scriptsize
\url{http://youtube.com/t/press\_statistics}} and millions of
surveillance cameras installed only in the UK. According to
Cisco\footnote{\scriptsize
\url{http://newsroom.cisco.com/dlls/2010/prod_060210.html}},
video is expected to dominate Internet traffic by 91\% in 2014.
%, or a webcam network installed for public monitoring of
Russian elections that has generated over 100 years of video
just in one
day\footnote{http://www.rostelecom.ru/en/ir/news/d212878}.

The access to information in such gigantic quantities of video
data requires accurate and efficient methods for automatic video
analysis. Much work has been recently devoted to automatic video
understanding and recognition of human actions in
particular~\cite{Laptev08,Liu11,Niebles10,Sadanand12,Schuldt04,Wang12}.
While the recognition accuracy has been continuously improved,
current methods remain limited to relatively small datasets due
to the low speed of video processing, often ranging in the order
of 1-2 frames per second. This stands in a sharp contrast with
the needs of large-scale video indexing and retrieval in modern
video archives. Fast video recognition is also required by
client applications, e.g., for automatic on-the-fly video
moderation and video editing. Compact video representations
enabling fast event recognition will also foster solutions to
new problems such as automatic clustering of very large video
collections.

The main goal of this work is efficient action recognition. We
follow the common bag-of-features action recognition
pipeline~\cite{Laptev08,Schuldt04,Wang12} and explore the speed
and memory trade-offs of its main steps, namely, feature
extraction, feature encoding and classification. Given their success for action recognition, we represent video
using motion-based HOF~\cite{Laptev08} and MBH~\cite{Wang12}
local descriptors. Motion estimation at dense grid, however, is a time-consuming process that
limits the speed of feature extraction.
In this work we eventually avoid motion estimation and design fast
descriptors using motion information from video compression.
In contrast to the dense optical flow (OF), video compression
provides sparse motion vectors only (here: MV~flow).
As one contribution of this paper, we show that the use of
sparse MV flow instead of the dense OF improves the speed of
feature extraction by {\em two orders of magnitude} and implies
only minor reduction in classification performance.

Feature encoding typically involves assignment of local
descriptors to one or several nearest elements in a visual
vocabulary. Given the large number of video descriptors, the
speed of this step is not negligible. We evaluate using kd-forest approximate nearest neighbor search \cite{Philbin07} and analyze the associated trade-off between the speed and
recognition accuracy. We next investigate Fisher vector (FV)
encoding~\cite{Perronnin12} and show improved action recognition
while using fast linear classifiers.
We evaluate the speed and accuracy of our approach on
\mbox{Hollywood-2}~\cite{Marszalek09}, UCF50~\cite{Reddy12}, HMDB51~\cite{Kuehne11} and UT-Interaction~\cite{Ryoo10} benchmarks.

The rest of the paper is organized as follows. 
After reviewing related work in Section~\ref{sec:relatedwork} we
address efficient extraction of local video features in
Section~\ref{sec:features}. Section~\ref{sec:quantization}
describes our fast and compact video encoding.
Section~\ref{sec:experiments} presents experimental results.



\section{Related work}
\label{sec:relatedwork}
Recent methods show significant progress towards action
recognition in realistic and challenging videos from YouTube,
movies and
TV~\cite{Laptev08,Laptev07,Liu11,Niebles10,Rodriguez08,Sadanand12,Wang12}.
Among other approaches, bag-of-features (BOF)
methods~\cite{Dollar05,Laptev05,Schuldt04} have gained
popularity due to their simplicity, wide range of application
and high recognition accuracy.
BOF methods represent actions by collections of local space-time
descriptors aggregated over the video.
Several alternative local descriptors have been proposed
including histograms of flow orientations (HOF)~\cite{Laptev08},
histograms of 3D gradients
(HOG3D)~\cite{klaser2008spatio,Scovanner07}, motion boundary
histograms (MBH)~\cite{Dalal06,Wang12}, shapes of point
trajectories~\cite{Matikainen09,Messing09,Wang12}, local trinity
patterns~\cite{Kliper12,Yeffet09} and others. Intermediate-level
features such as action attributes~\cite{Liu11} and action
bank~\cite{Sadanand12} have also been explored. Recent
comprehensive evaluation~\cite{Wang12} suggests that MBH, HOF
and HOG descriptors sampled along dense point trajectories
result in great performance improving accuracy of other
methods on a number of challenging datasets~\cite{Wang12}. Most
of the previous work, however, does not address computational
efficiency required for larger scale problems. Random sampling of dense features locations~\cite{Feng13} has been studied as well, it leads to feature extraction substantially faster than \cite{Wang12}, but several times slower than this work. We
follow~\cite{Wang12} and design a new motion-based local
descriptor that drastically improves the speed of previous
methods at the cost of minor decrease in the recognition
accuracy.

Efficient action recognition has been addressed by several
methods. The work~\cite{mpeg3,mpeg2,mpeg1} is particularly
related to ours as it makes use of motion information from video
compression for fast action recognition. This previous work,
however, designs action-specific descriptors and, hence, its
speed scales linearly with the number of action classes. In
contrast, we design a generic action representation and evaluate
its accuracy and efficiency on many action classes in
challenging settings.
Yeffet and Wolf~\cite{Yeffet09} extend the fast LBP image
descriptor to a Local Trinity Pattern (LTP) descriptor in video
and evaluate its accuracy on action recognition. While LTP was
claimed to run in real time, no quantitative evaluation of its
speed was reported in~\cite{Yeffet09}. Differently to LTP, we
use flow-based MBH and HOF descriptors which have recently shown
excellent results for action recognition~\cite{Wang12}. Yu
et~al.~\cite{Yu10} have proposed another pixel-based local
descriptor for efficient action recognition. We quantitatively
compare our method with~\cite{Yu10} and show improvements in
both the speed and accuracy.

Alternative schemes for feature encoding
%, i.e.~aggregation of local descriptors into global
representations,
have been recently evaluated for image classification
in~\cite{Chatfield11}.
%While histogram encoding is the most common one, 
Fisher vector (FV) encoding~\cite{Perronnin10} has been shown to
provide best accuracy using efficient linear kernels for
classification. FV encoding has been successfully applied for event detection \cite{Revaud13} and we are confirming its improved performance and efficiency compared to the
histogram encoding typically used in action recognition. We also
investigate efficient computation of FV using approximate
nearest neighbor methods for descriptor assignment.

FV encoding enables high recognition accuracy using fast linear
classifiers which is a big advantage for large-scale video
recognition. The dimension of FV, however, is very large (about
$2.5\cdot10^6$ in our implementation) and implies high
requirements on the memory. Compression of FV and other types of
encodings has been studied for instance-level image
search~\cite{Jegou12}. The dimensionality reduction of FV has
also been recently addressed for large-scale image
classification problems~\cite{Mensink12,Perronnin12,Sanchez13}.
An extensive review of methods for dimensionality reduction for
image retrieval is given by \cite{Grauman13}.
In this work we do not tackle the dimensionality problem of the Fisher vectors.

\section{Bag-of-features action recognition}
A typical bag-of-features (BOF) action recognition pipeline consists of the three stages:
\begin{enumerate}
	\item Feature extraction. Select video regions are summarized by a number of descriptors such as HOF or MBH.
	\item Feature encoding. The descriptor set is converted to a fixed-size vector. The popular choices are the histogram encoding, the VLAD encoding, and the FV encoding.
	\item Classification. Usually, SVM is used with the $\chi^2$-kernel for histograms or the linear kernel for VLAD and FV.
\end{enumerate}

The analysis of the state-of-the-art BOF system~\cite{Wang12} indicates that most of the running time ($52\%$) is spent on the computation of optical flow, while the second most expensive operation ($26\%$) is aggregation of dense flow measurements into histogram descriptors. In this paper we alleviate the expense of both of these steps by (i) reusing motion estimates available from video compression and (ii) constructing descriptors from very sparse motion measurements.

\section{Local motion features}
\label{sec:features}
Our motion descriptors summarize cuboid video volumes, the detailed layout is described below.

\subsection{Descriptor design}
We follow the design of previously proposed space-time descriptors~\cite{Laptev08,Wang12} and define our descriptor by histograms of MPEG flow vectors accumulated in a video patch. Each patch is divided into cells as illustrated in Figure~\ref{fig:CDdescriptor} and the normalized histograms from patch cells are concatenated into a descriptor vector. Constrained by the coarse $16\times16$ pixel spatial resolution of MPEG flow, we align descriptor grid cells with positions of motion vectors (red points in Figure~\ref{fig:CDdescriptor}). We also use bilinear interpolation of flow and increase the spatial resolution of the flow field by the factor of two (yellow points in Figure~\ref{fig:CDdescriptor})\footnote{We found interpolation of the flow to be important in our experiments}. 

Following~\cite{Wang12}, we compute HOF descriptors as histograms of MPEG flow discretized into eight orientation bins and a no-motion bin. For MBHx and MBHy descriptors the spatial gradients of the $v_x$ and $v_y$ components of the flow are similarly discretized into nine bins. %We compute HOF, MBHx, MBHy histograms by accumulating quantized flow values within each cell (see Figure~\ref{fig:CDdescriptor}). 
The final descriptor is obtained by concatenating histograms from each cell of the $2\times2\times3$ descriptor grid followed by $l_2$-normalization of every temporal slice. HOG descriptors are computed at the same sparse set of points.

The above scheme defines a $32\times32\times15$ pixel descriptor which we compute at every location of the video defined by the spatial stride of $16$ pixels and temporal stride of $5$ frames. To sample multiple spatial scales, we similarly define a $48\times48\times15$ pixel descriptor sampled with $24$ pixels spatial stride. For a video of $640\times480$ pixels spatial resolution we obtain around 300 descriptors per frame. This is comparable to $\approx350$ dense trajectory features produced by the method in~\cite{Wang12} for the same video resolution.

Main computational advantages of our descriptor originate from the elimination of the optical flow computation and from the coarse sampling of flow vectors. As we demonstrate experimentally in Section~\ref{sec:experiments}, these modifications imply drastic improvements of computational requirements at the cost of minor reduction in the classification performance. 
\subsection{Features from sparse optical flow}
Optical flow occupies a large portion of time in
descriptor computation. In order to achieve faster than
real-time
performance we evaluate a natural speed-up - estimating optical
flow
at a smaller point set.

\subsection{Efficient features from compressed video}
Motion vectors + figure... Evaluation on BOF/histograms on
UCF50/HMDB/HWD/UTI. Codecs, channels. Breakdown.


\subsection{Role of trajectories}
Discussion of V*/V0.


\section{Descriptor encoding}
\label{sec:quantization}
Overview of encoding methods.

\section{Experimental evaluation}
\label{sec:experiments}
Evaluation of VLAD / FV with different parameters. Breakdown.


\section{Conclusions}




{
\small
\bibliographystyle{cvpr2014authorKit_latex/ieee}
\bibliography{shortstrings,cvpr14}
}

\end{document}
>>>>>>> Stashed changes
